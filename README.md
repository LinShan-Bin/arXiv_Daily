# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2025-03-01

| **Title** | **Date** | **Abstract** | **Comment** | **Summary** |
| --- | --- | --- | --- | --- |
| **[LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language Grounding](http://arxiv.org/abs/2502.20389v1)** | 2025-02-27 | <details><summary>Show</summary><p>Our approach to training 3D vision-language understanding models is to train a feedforward model that makes predictions in 3D, but never requires 3D labels and is supervised only in 2D, using 2D losses and differentiable rendering. The approach is new for vision-language understanding. By treating the reconstruction as a ``latent variable'', we can render the outputs without placing unnecessary constraints on the network architecture (e.g. can be used with decoder-only models). For training, only need images and camera pose, and 2D labels. We show that we can even remove the need for 2D labels by using pseudo-labels from pretrained 2D models. We demonstrate this to pretrain a network, and we finetune it for 3D vision-language understanding tasks. We show this approach outperforms baselines/sota for 3D vision-language grounding, and also outperforms other 3D pretraining techniques. Project page: https://liftgs.github.io.</p></details> | <details><summary>Proje...</summary><p>Project page: https://liftgs.github.io</p></details> | <details><summary>Show</summary><p>论文提出LIFT-GS方法训练3D视觉语言理解模型，无需3D标签，仅用2D损失和可微渲染监督，将重建视为“潜在变量”，还可去除2D标签，实验表明该方法在3D视觉语言基础任务中优于基线和其他预训练技术。 </p></details> |
| **[InsTaG: Learning Personalized 3D Talking Head from Few-Second Video](http://arxiv.org/abs/2502.20387v1)** | 2025-02-27 | <details><summary>Show</summary><p>Despite exhibiting impressive performance in synthesizing lifelike personalized 3D talking heads, prevailing methods based on radiance fields suffer from high demands for training data and time for each new identity. This paper introduces InsTaG, a 3D talking head synthesis framework that allows a fast learning of realistic personalized 3D talking head from few training data. Built upon a lightweight 3DGS person-specific synthesizer with universal motion priors, InsTaG achieves high-quality and fast adaptation while preserving high-level personalization and efficiency. As preparation, we first propose an Identity-Free Pre-training strategy that enables the pre-training of the person-specific model and encourages the collection of universal motion priors from long-video data corpus. To fully exploit the universal motion priors to learn an unseen new identity, we then present a Motion-Aligned Adaptation strategy to adaptively align the target head to the pre-trained field, and constrain a robust dynamic head structure under few training data. Experiments demonstrate our outstanding performance and efficiency under various data scenarios to render high-quality personalized talking heads.</p></details> | <details><summary>Accep...</summary><p>Accepted at CVPR 2025. Project page: https://fictionarry.github.io/InsTaG/</p></details> | <details><summary>Show</summary><p>论文介绍了InsTaG这一3D说话头像合成框架，能从少量训练数据快速学习逼真个性化头像。基于轻量级合成器，提出无身份预训练和运动对齐适应策略，实验证明其在多数据场景下表现出色、效率高。 </p></details> |
| **[G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation](http://arxiv.org/abs/2411.18369v2)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advances in imitation learning for 3D robotic manipulation have shown promising results with diffusion-based policies. However, achieving human-level dexterity requires seamless integration of geometric precision and semantic understanding. We present G3Flow, a novel framework that constructs real-time semantic flow, a dynamic, object-centric 3D semantic representation by leveraging foundation models. Our approach uniquely combines 3D generative models for digital twin creation, vision foundation models for semantic feature extraction, and robust pose tracking for continuous semantic flow updates. This integration enables complete semantic understanding even under occlusions while eliminating manual annotation requirements. By incorporating semantic flow into diffusion policies, we demonstrate significant improvements in both terminal-constrained manipulation and cross-object generalization. Extensive experiments across five simulation tasks show that G3Flow consistently outperforms existing approaches, achieving up to 68.3% and 50.1% average success rates on terminal-constrained manipulation and cross-object generalization tasks respectively. Our results demonstrate the effectiveness of G3Flow in enhancing real-time dynamic semantic feature understanding for robotic manipulation policies.</p></details> | <details><summary>Webpa...</summary><p>Webpage: https://tianxingchen.github.io/G3Flow/, accepted to CVPR 2025</p></details> | <details><summary>Show</summary><p>该论文提出G3Flow框架，利用基础模型构建实时语义流，独特整合3D生成模型、视觉基础模型和姿态跟踪。能在遮挡下实现语义理解，消除人工标注。融入扩散策略后，在相关任务中显著优于现有方法，提升机器人操作语义特征理解。 </p></details> |
| **[ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model](http://arxiv.org/abs/2502.20323v1)** | 2025-02-27 | <details><summary>Show</summary><p>Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips. Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential. In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook. Furthermore, our model can adapt to unseen speaking styles using sample motion sequences, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality.</p></details> | <details><summary>More ...</summary><p>More video demonstrations, code, models and data can be found on our project website: http://xg-chu.site/project_artalk/</p></details> | <details><summary>Show</summary><p>该论文提出一种基于自回归模型的ARTalk，用于语音驱动3D头部动画。能实时生成高度同步的嘴唇动作、逼真头部姿势和眨眼，还可适应未见说话风格，在唇同步精度和感知质量上优于现有方法。 </p></details> |
| **[Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds](http://arxiv.org/abs/2502.20316v1)** | 2025-02-27 | <details><summary>Show</summary><p>Masked autoencoders (MAE) have shown tremendous potential for self-supervised learning (SSL) in vision and beyond. However, point clouds from LiDARs used in automated driving are particularly challenging for MAEs since large areas of the 3D volume are empty. Consequently, existing work suffers from leaking occupancy information into the decoder and has significant computational complexity, thereby limiting the SSL pre-training to only 2D bird's eye view encoders in practice. In this work, we propose the novel neighborhood occupancy MAE (NOMAE) that overcomes the aforementioned challenges by employing masked occupancy reconstruction only in the neighborhood of non-masked voxels. We incorporate voxel masking and occupancy reconstruction at multiple scales with our proposed hierarchical mask generation technique to capture features of objects of different sizes in the point cloud. NOMAEs are extremely flexible and can be directly employed for SSL in existing 3D architectures. We perform extensive evaluations on the nuScenes and Waymo Open datasets for the downstream perception tasks of semantic segmentation and 3D object detection, comparing with both discriminative and generative SSL methods. The results demonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for multiple point cloud perception tasks.</p></details> |  | <details><summary>Show</summary><p>该论文针对LiDAR点云自监督学习，提出新型邻域占用掩蔽自编码器（NOMAE）。通过仅在未掩蔽体素邻域进行占用重建等克服挑战，多尺度灵活且能用于现有3D架构，实验表明其在多任务上达新SOTA。 </p></details> |
| **[LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language Grounding](http://arxiv.org/abs/2502.20389v1)** | 2025-02-27 | <details><summary>Show</summary><p>Our approach to training 3D vision-language understanding models is to train a feedforward model that makes predictions in 3D, but never requires 3D labels and is supervised only in 2D, using 2D losses and differentiable rendering. The approach is new for vision-language understanding. By treating the reconstruction as a ``latent variable'', we can render the outputs without placing unnecessary constraints on the network architecture (e.g. can be used with decoder-only models). For training, only need images and camera pose, and 2D labels. We show that we can even remove the need for 2D labels by using pseudo-labels from pretrained 2D models. We demonstrate this to pretrain a network, and we finetune it for 3D vision-language understanding tasks. We show this approach outperforms baselines/sota for 3D vision-language grounding, and also outperforms other 3D pretraining techniques. Project page: https://liftgs.github.io.</p></details> | <details><summary>Proje...</summary><p>Project page: https://liftgs.github.io</p></details> | <details><summary>Show</summary><p>论文提出LIFT-GS方法训练3D视觉语言理解模型，无需3D标签，仅用2D损失和可微渲染监督，将重建视为“潜在变量”，甚至能用预训练2D模型伪标签，在3D视觉语言基础任务中优于基线和其他预训练技术。 </p></details> |
| **[Tight Inversion: Image-Conditioned Inversion for Real Image Editing](http://arxiv.org/abs/2502.20376v1)** | 2025-02-27 | <details><summary>Show</summary><p>Text-to-image diffusion models offer powerful image editing capabilities. To edit real images, many methods rely on the inversion of the image into Gaussian noise. A common approach to invert an image is to gradually add noise to the image, where the noise is determined by reversing the sampling equation. This process has an inherent tradeoff between reconstruction and editability, limiting the editing of challenging images such as highly-detailed ones. Recognizing the reliance of text-to-image models inversion on a text condition, this work explores the importance of the condition choice. We show that a condition that precisely aligns with the input image significantly improves the inversion quality. Based on our findings, we introduce Tight Inversion, an inversion method that utilizes the most possible precise condition -- the input image itself. This tight condition narrows the distribution of the model's output and enhances both reconstruction and editability. We demonstrate the effectiveness of our approach when combined with existing inversion methods through extensive experiments, evaluating the reconstruction accuracy as well as the integration with various editing methods.</p></details> | <details><summary>Proje...</summary><p>Project page at: https://tight-inversion.github.io</p></details> | <details><summary>Show</summary><p>论文指出文本到图像扩散模型编辑真实图像时，常见反演方法有局限。研究发现精确匹配输入图像的条件可提升反演质量，据此提出Tight Inversion方法，用输入图像自身作条件，增强了重建与可编辑性，实验验证了其有效性。 </p></details> |
| **[Removing Neural Signal Artifacts with Autoencoder-Targeted Adversarial Transformers (AT-AT)](http://arxiv.org/abs/2502.05332v2)** | 2025-02-27 | <details><summary>Show</summary><p>Electromyogenic (EMG) noise is a major contamination source in EEG data that can impede accurate analysis of brain-specific neural activity. Recent literature on EMG artifact removal has moved beyond traditional linear algorithms in favor of machine learning-based systems. However, existing deep learning-based filtration methods often have large compute footprints and prohibitively long training times. In this study, we present a new machine learning-based system for filtering EMG interference from EEG data using an autoencoder-targeted adversarial transformer (AT-AT). By leveraging the lightweight expressivity of an autoencoder to determine optimal time-series transformer application sites, our AT-AT architecture achieves a >90% model size reduction compared to published artifact removal models. The addition of adversarial training ensures that filtered signals adhere to the fundamental characteristics of EEG data. We trained AT-AT using published neural data from 67 subjects and found that the system was able to achieve comparable test performance to larger models; AT-AT posted a mean reconstructive correlation coefficient above 0.95 at an initial signal-to-noise ratio (SNR) of 2 dB and 0.70 at -7 dB SNR. Further research generalizing these results to broader sample sizes beyond these isolated test cases will be crucial; while outside the scope of this study, we also include results from a real-world deployment of AT-AT in the Appendix.</p></details> | <details><summary>Accep...</summary><p>Accepted at CNS 2025, Boston, MA, USA</p></details> | <details><summary>Show</summary><p>该论文提出用自动编码器靶向对抗变压器（AT-AT）从脑电图数据中过滤肌电干扰的新机器学习系统。利用自动编码器确定变压器应用位点，模型尺寸大幅减小，对抗训练确保滤波信号符合脑电数据特征，效果与更大模型相当。 </p></details> |
| **[T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration](http://arxiv.org/abs/2502.20333v1)** | 2025-02-27 | <details><summary>Show</summary><p>Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times. Code for reproducing all results will be made publicly available upon publication.</p></details> |  | <details><summary>Show</summary><p>论文提出T1-PILOT，将T1信号弛豫模型纳入采样重建框架，用于指导非笛卡尔轨迹学习等。通过实验表明其显著优于基线策略，能在更高加速因子下实现更高T1图保真度，优化轨迹与弛豫模型可提升精度并减少采集时间。 </p></details> |
| **[Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds](http://arxiv.org/abs/2502.20316v1)** | 2025-02-27 | <details><summary>Show</summary><p>Masked autoencoders (MAE) have shown tremendous potential for self-supervised learning (SSL) in vision and beyond. However, point clouds from LiDARs used in automated driving are particularly challenging for MAEs since large areas of the 3D volume are empty. Consequently, existing work suffers from leaking occupancy information into the decoder and has significant computational complexity, thereby limiting the SSL pre-training to only 2D bird's eye view encoders in practice. In this work, we propose the novel neighborhood occupancy MAE (NOMAE) that overcomes the aforementioned challenges by employing masked occupancy reconstruction only in the neighborhood of non-masked voxels. We incorporate voxel masking and occupancy reconstruction at multiple scales with our proposed hierarchical mask generation technique to capture features of objects of different sizes in the point cloud. NOMAEs are extremely flexible and can be directly employed for SSL in existing 3D architectures. We perform extensive evaluations on the nuScenes and Waymo Open datasets for the downstream perception tasks of semantic segmentation and 3D object detection, comparing with both discriminative and generative SSL methods. The results demonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for multiple point cloud perception tasks.</p></details> |  | <details><summary>Show</summary><p>该论文针对LiDAR点云自监督学习中现有MAE的问题，提出新颖的邻域占用MAE（NOMAE）。通过在非掩码体素邻域进行占用重建，多尺度结合层次掩码生成技术，灵活用于3D架构，在多任务基准测试中达新SOTA。 </p></details> |
| **[Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids](http://arxiv.org/abs/2502.20396v1)** | 2025-02-27 | <details><summary>Show</summary><p>Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.</p></details> | <details><summary>Proje...</summary><p>Project page can be found at https://toruowo.github.io/recipe/</p></details> | <details><summary>Show</summary><p>该论文研究强化学习用于类人机器人灵巧操作的挑战，介绍新技术并验证。贡献有自动实到仿调优模块、通用奖励设计等。在三项任务中获成果，通过消融研究展示各技术作用，实现无人类示范的类人灵巧操作学习。 </p></details> |
| **[Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation](http://arxiv.org/abs/2502.20391v1)** | 2025-02-27 | <details><summary>Show</summary><p>Building robotic agents capable of operating across diverse environments and object types remains a significant challenge, often requiring extensive data collection. This is particularly restrictive in robotics, where each data point must be physically executed in the real world. Consequently, there is a critical need for alternative data sources for robotics and frameworks that enable learning from such data. In this work, we present Point Policy, a new method for learning robot policies exclusively from offline human demonstration videos and without any teleoperation data. Point Policy leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through semantically meaningful key points. This approach yields a morphology-agnostic representation that facilitates effective policy learning. Our experiments on 8 real-world tasks demonstrate an overall 75% absolute improvement over prior works when evaluated in identical settings as training. Further, Point Policy exhibits a 74% gain across tasks for novel object instances and is robust to significant background clutter. Videos of the robot are best viewed at https://point-policy.github.io/.</p></details> |  | <details><summary>Show</summary><p>论文提出Point Policy，能仅从离线人类演示视频学习机器人策略，无需遥操作数据。利用先进视觉模型和策略架构，通过关键点捕捉物体状态，将人类手部姿态转化为机器人姿态，实验显示相比 prior works有显著提升。 </p></details> |
| **[Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization](http://arxiv.org/abs/2502.20382v1)** | 2025-02-27 | <details><summary>Show</summary><p>We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline's effectiveness by training diffusion policies from the generated datasets for challenging contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bimanual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input. Project website: https://lujieyang.github.io/physicsgen/.</p></details> |  | <details><summary>Show</summary><p>该论文提出低成本数据生成管道，融合物理模拟、人类演示和基于模型的规划，为接触丰富的机器人操作生成大规模高质量数据集。创新点在于能跨机器人实体和物理参数优化演示，实现跨实体数据转移及旧数据集复用。 </p></details> |
| **[Waves and symbols in neuromorphic hardware: from analog signal processing to digital computing on the same computational substrate](http://arxiv.org/abs/2502.20381v1)** | 2025-02-27 | <details><summary>Show</summary><p>Neural systems use the same underlying computational substrate to carry out analog filtering and signal processing operations, as well as discrete symbol manipulation and digital computation. Inspired by the computational principles of canonical cortical microcircuits, we propose a framework for using recurrent spiking neural networks to seamlessly and robustly switch between analog signal processing and categorical and discrete computation. We provide theoretical analysis and practical neural network design tools to formally determine the conditions for inducing this switch. We demonstrate the robustness of this framework experimentally with hardware soft Winner-Take-All and mixed-feedback recurrent spiking neural networks, implemented by appropriately configuring the analog neuron and synapse circuits of a mixed-signal neuromorphic processor chip.</p></details> |  | <details><summary>Show</summary><p>受皮层微电路计算原理启发，提出用递归脉冲神经网络在模拟信号处理与离散计算间无缝切换的框架。提供理论分析与设计工具，通过硬件实验证明其鲁棒性，能在同一计算基板上实现从模拟到数字计算的转换。 </p></details> |
| **[G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation](http://arxiv.org/abs/2411.18369v2)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advances in imitation learning for 3D robotic manipulation have shown promising results with diffusion-based policies. However, achieving human-level dexterity requires seamless integration of geometric precision and semantic understanding. We present G3Flow, a novel framework that constructs real-time semantic flow, a dynamic, object-centric 3D semantic representation by leveraging foundation models. Our approach uniquely combines 3D generative models for digital twin creation, vision foundation models for semantic feature extraction, and robust pose tracking for continuous semantic flow updates. This integration enables complete semantic understanding even under occlusions while eliminating manual annotation requirements. By incorporating semantic flow into diffusion policies, we demonstrate significant improvements in both terminal-constrained manipulation and cross-object generalization. Extensive experiments across five simulation tasks show that G3Flow consistently outperforms existing approaches, achieving up to 68.3% and 50.1% average success rates on terminal-constrained manipulation and cross-object generalization tasks respectively. Our results demonstrate the effectiveness of G3Flow in enhancing real-time dynamic semantic feature understanding for robotic manipulation policies.</p></details> | <details><summary>Webpa...</summary><p>Webpage: https://tianxingchen.github.io/G3Flow/, accepted to CVPR 2025</p></details> | <details><summary>Show</summary><p>该论文提出G3Flow框架，利用基础模型构建实时语义流，独特结合多种模型，能在遮挡下实现语义理解且无需手动标注。融入语义流到扩散策略中，在多任务实验中显著优于现有方法，提升机器人操作语义理解能力。 </p></details> |
| **[SecureGaze: Defending Gaze Estimation Against Backdoor Attacks](http://arxiv.org/abs/2502.20306v1)** | 2025-02-27 | <details><summary>Show</summary><p>Gaze estimation models are widely used in applications such as driver attention monitoring and human-computer interaction. While many methods for gaze estimation exist, they rely heavily on data-hungry deep learning to achieve high performance. This reliance often forces practitioners to harvest training data from unverified public datasets, outsource model training, or rely on pre-trained models. However, such practices expose gaze estimation models to backdoor attacks. In such attacks, adversaries inject backdoor triggers by poisoning the training data, creating a backdoor vulnerability: the model performs normally with benign inputs, but produces manipulated gaze directions when a specific trigger is present. This compromises the security of many gaze-based applications, such as causing the model to fail in tracking the driver's attention. To date, there is no defense that addresses backdoor attacks on gaze estimation models. In response, we introduce SecureGaze, the first solution designed to protect gaze estimation models from such attacks. Unlike classification models, defending gaze estimation poses unique challenges due to its continuous output space and globally activated backdoor behavior. By identifying distinctive characteristics of backdoored gaze estimation models, we develop a novel and effective approach to reverse-engineer the trigger function for reliable backdoor detection. Extensive evaluations in both digital and physical worlds demonstrate that SecureGaze effectively counters a range of backdoor attacks and outperforms seven state-of-the-art defenses adapted from classification models.</p></details> |  | <details><summary>Show</summary><p>论文指出凝视估计模型因依赖数据易遭后门攻击，目前无防御方法。为此提出SecureGaze，通过识别后门模型特征，开发反向工程触发函数的方法检测后门，经评估能有效抵御多种攻击，优于现有七种基于分类模型的防御。 </p></details> |
| **[The Role of Tactile Sensing for Learning Reach and Grasp](http://arxiv.org/abs/2502.20367v1)** | 2025-02-27 | <details><summary>Show</summary><p>Stable and robust robotic grasping is essential for current and future robot applications. In recent works, the use of large datasets and supervised learning has enhanced speed and precision in antipodal grasping. However, these methods struggle with perception and calibration errors due to large planning horizons. To obtain more robust and reactive grasping motions, leveraging reinforcement learning combined with tactile sensing is a promising direction. Yet, there is no systematic evaluation of how the complexity of force-based tactile sensing affects the learning behavior for grasping tasks. This paper compares various tactile and environmental setups using two model-free reinforcement learning approaches for antipodal grasping. Our findings suggest that under imperfect visual perception, various tactile features improve learning outcomes, while complex tactile inputs complicate training.</p></details> |  | <details><summary>Show</summary><p>论文主要研究触觉传感在学习抓取中的作用。通过两种无模型强化学习方法比较不同触觉和环境设置，发现 imperfect 视觉感知下，多种触觉特征可改善抓取学习结果，复杂触觉输入使训练变难，为相关研究提供新见解。 </p></details> |
| **[Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids](http://arxiv.org/abs/2502.20396v1)** | 2025-02-27 | <details><summary>Show</summary><p>Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.</p></details> | <details><summary>Proje...</summary><p>Project page can be found at https://toruowo.github.io/recipe/</p></details> | <details><summary>Show</summary><p>该论文研究强化学习用于人形机器人灵巧操作的挑战，提出自动化实到模拟调整模块、广义奖励设计等创新技术。经实验验证，在三项人形灵巧操作任务中取得成果，无需人类示范即可实现鲁棒泛化与高性能。 </p></details> |
| **[Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization](http://arxiv.org/abs/2502.20382v1)** | 2025-02-27 | <details><summary>Show</summary><p>We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline's effectiveness by training diffusion policies from the generated datasets for challenging contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bimanual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input. Project website: https://lujieyang.github.io/physicsgen/.</p></details> |  | <details><summary>Show</summary><p>该论文提出低成本数据生成管道，融合物理模拟、人类示范和基于模型的规划，从少量人类示范出发，经优化生成跨机器人实例和物理参数的高质量数据集，验证其有效性，实现跨实例数据转移及零样本硬件部署。 </p></details> |
| **[ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting](http://arxiv.org/abs/2502.20386v1)** | 2025-02-27 | <details><summary>Show</summary><p>We address the challenge of task-oriented navigation in unstructured and unknown environments, where robots must incrementally build and reason on rich, metric-semantic maps in real time. Since tasks may require clarification or re-specification, it is necessary for the information in the map to be rich enough to enable generalization across a wide range of tasks. To effectively execute tasks specified in natural language, we propose a hierarchical representation built on language-embedded Gaussian splatting that enables both sparse semantic planning that lends itself to online operation and dense geometric representation for collision-free navigation. We validate the effectiveness of our method through real-world robot experiments conducted in both cluttered indoor and kilometer-scale outdoor environments, with a competitive ratio of about 60% against privileged baselines. Experiment videos and more details can be found on our project page: https://atlasnav.github.io</p></details> |  | <details><summary>Show</summary><p>该论文旨在解决非结构化未知环境中任务导向导航问题。提出基于语言嵌入高斯点云的分层表示，兼具稀疏语义规划与密集几何表示，能跨任务执行自然语言指定任务，通过实验验证方法有效性，相比基线有约60%竞争力。 </p></details> |
| **[Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](http://arxiv.org/abs/2502.20383v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.</p></details> | <details><summary>Proje...</summary><p>Project website: http://vulnerable-ai-agents.github.io</p></details> | <details><summary>Show</summary><p>该论文研究为何网络人工智能代理比独立语言模型更易受攻击。发现虽基于相同安全模型，但网络代理因多种差异及复杂信号更脆弱。提出组件级分析和系统评估框架，确定三个放大其脆弱性的关键因素，强调加强人工智能代理安全设计的必要性。 </p></details> |
| **[Multi-Agent Path Planning in Complex Environments using Gaussian Belief Propagation with Global Path Finding](http://arxiv.org/abs/2502.20369v1)** | 2025-02-27 | <details><summary>Show</summary><p>Multi-agent path planning is a critical challenge in robotics, requiring agents to navigate complex environments while avoiding collisions and optimizing travel efficiency. This work addresses the limitations of existing approaches by combining Gaussian belief propagation with path integration and introducing a novel tracking factor to ensure strict adherence to global paths. The proposed method is tested with two different global path-planning approaches: rapidly exploring random trees and a structured planner, which leverages predefined lane structures to improve coordination. A simulation environment was developed to validate the proposed method across diverse scenarios, each posing unique challenges in navigation and communication. Simulation results demonstrate that the tracking factor reduces path deviation by 28% in single-agent and 16% in multi-agent scenarios, highlighting its effectiveness in improving multi-agent coordination, especially when combined with structured global planning.</p></details> | <details><summary>Accep...</summary><p>Accepted by "International Conference on Robotics and Automation" - ICRA 2025</p></details> | <details><summary>Show</summary><p>该论文针对机器人多智能体路径规划难题，结合高斯信念传播与路径积分，引入新跟踪因子克服现有方法局限。用两种全局路径规划法测试，开发模拟环境验证。结果表明跟踪因子有效提升多智能体协调，尤其结合结构化全局规划时。 </p></details> |
| **[Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](http://arxiv.org/abs/2502.20364v1)** | 2025-02-27 | <details><summary>Show</summary><p>Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.</p></details> | <details><summary>10 pa...</summary><p>10 pages, 6 figures, 5 tables</p></details> | <details><summary>Show</summary><p>该论文介绍一种融合检索增强生成、向量存储和知识图谱的生成式人工智能系统，通过非负矩阵分解构建，用于法律领域。能系统收集法律文本，弥合传统搜索与语境理解差距，支持多种任务，推动计算法学和人工智能发展。 </p></details> |
| **[Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application](http://arxiv.org/abs/2502.20326v1)** | 2025-02-27 | <details><summary>Show</summary><p>This paper proposes a holistic framework for autonomous guidance, navigation, and task distribution among multi-drone systems operating in Global Navigation Satellite System (GNSS)-denied indoor settings. We advocate for a Deep Reinforcement Learning (DRL)-based guidance mechanism, utilising the Twin Delayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency of the training process, we incorporate an Artificial Potential Field (APF)-based reward structure, enabling the agent to refine its movements, thereby promoting smoother paths and enhanced obstacle avoidance in indoor contexts. Furthermore, we tackle the issue of task distribution among cooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This GCN represents the interactions between drones and tasks, facilitating dynamic and real-time task allocation that reflects the current environmental conditions and the capabilities of the drones. Such an approach fosters effective coordination and collaboration among multiple drones during search and rescue operations or other exploratory endeavours. Lastly, to ensure precise odometry in environments lacking GNSS, we employ Light Detection And Ranging Simultaneous Localisation and Mapping complemented by a depth camera to mitigate the hallway problem. This integration offers robust localisation and mapping functionalities, thereby enhancing the systems dependability in indoor navigation. The proposed multi-drone framework not only elevates individual navigation capabilities but also optimises coordinated task allocation in complex, obstacle-laden environments. Experimental evaluations conducted in a setup tailored to meet the requirements of the NATO Sapience Autonomous Cooperative Drone Competition demonstrate the efficacy of the proposed system, yielding outstanding results and culminating in a first-place finish in the 2024 Sapience competition.</p></details> | 18 Pages, 21 Figures | <details><summary>Show</summary><p>该论文针对全球导航卫星系统（GNSS）信号被拒的室内环境，提出基于深度强化学习的多无人机自主决策框架。采用双延迟深度确定性策略梯度算法，结合人工势场奖励结构，用图卷积网络解决任务分配，还利用激光雷达和深度相机增强室内定位，实验证明效果良好。 </p></details> |
| **[On Adversarial Attacks In Acoustic Drone Localization](http://arxiv.org/abs/2502.20325v1)** | 2025-02-27 | <details><summary>Show</summary><p>Multi-rotor aerial autonomous vehicles (MAVs, more widely known as "drones") have been generating increased interest in recent years due to their growing applicability in a vast and diverse range of fields (e.g., agriculture, commercial delivery, search and rescue). The sensitivity of visual-based methods to lighting conditions and occlusions had prompted growing study of navigation reliant on other modalities, such as acoustic sensing. A major concern in using drones in scale for tasks in non-controlled environments is the potential threat of adversarial attacks over their navigational systems, exposing users to mission-critical failures, security breaches, and compromised safety outcomes that can endanger operators and bystanders. While previous work shows impressive progress in acoustic-based drone localization, prior research in adversarial attacks over drone navigation only addresses visual sensing-based systems. In this work, we aim to compensate for this gap by supplying a comprehensive analysis of the effect of PGD adversarial attacks over acoustic drone localization. We furthermore develop an algorithm for adversarial perturbation recovery, capable of markedly diminishing the affect of such attacks in our setting. The code for reproducing all experiments will be released upon publication.</p></details> |  | <details><summary>Show</summary><p>该论文关注多旋翼无人机声学定位中的对抗攻击。此前相关研究仅针对视觉传感系统，本文全面分析PGD攻击对声学无人机定位的影响，并开发对抗扰动恢复算法，以弥补这一空白，实验代码将在发表时发布。 </p></details> |
| **[R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts](http://arxiv.org/abs/2502.20395v1)** | 2025-02-27 | <details><summary>Show</summary><p>In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method "Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.</p></details> |  | <details><summary>Show</summary><p>论文指出大型多模态模型中视觉等非语言模态感知不如语言模型推理能力，影响其在下游任务表现。提出R2-T2方法，在测试时局部优化路由权重向量，有三种策略，能大幅提升多模态模型在多样任务基准上的性能，且无需训练基础模型参数。 </p></details> |
| **[ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting](http://arxiv.org/abs/2502.20386v1)** | 2025-02-27 | <details><summary>Show</summary><p>We address the challenge of task-oriented navigation in unstructured and unknown environments, where robots must incrementally build and reason on rich, metric-semantic maps in real time. Since tasks may require clarification or re-specification, it is necessary for the information in the map to be rich enough to enable generalization across a wide range of tasks. To effectively execute tasks specified in natural language, we propose a hierarchical representation built on language-embedded Gaussian splatting that enables both sparse semantic planning that lends itself to online operation and dense geometric representation for collision-free navigation. We validate the effectiveness of our method through real-world robot experiments conducted in both cluttered indoor and kilometer-scale outdoor environments, with a competitive ratio of about 60% against privileged baselines. Experiment videos and more details can be found on our project page: https://atlasnav.github.io</p></details> |  | <details><summary>Show</summary><p>该论文旨在应对非结构化未知环境中任务导向导航挑战。提出基于语言嵌入高斯平铺的分层表示，可实时构建和推理地图，能进行稀疏语义规划与密集几何表示，通过实验验证了方法有效性，相比基线有约60%的竞争力。 </p></details> |
| **[PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation](http://arxiv.org/abs/2502.20377v1)** | 2025-02-27 | <details><summary>Show</summary><p>High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at https://github.com/kilian-group/phantom-wiki.</p></details> |  | <details><summary>Show</summary><p>该论文提出PhantomWiki，这是一种按需生成独特、事实一致文档语料库及问答对的管道。与以往不同，每次评估按需生成新实例，能分别通过改变问题难度和语料库大小来评估推理与检索能力，贡献了抗数据泄露的评估框架。 </p></details> |
| **[Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](http://arxiv.org/abs/2502.20364v1)** | 2025-02-27 | <details><summary>Show</summary><p>Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.</p></details> | <details><summary>10 pa...</summary><p>10 pages, 6 figures, 5 tables</p></details> | <details><summary>Show</summary><p>该论文介绍一种整合检索增强生成（RAG）、向量存储（VS）和知识图谱（KG）的生成式人工智能系统，通过非负矩阵分解构建，用于法律信息检索等，能增强AI推理、减少幻觉，助力法律研究与预测法律趋势。 </p></details> |
| **[Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs](http://arxiv.org/abs/2502.20356v1)** | 2025-02-27 | <details><summary>Show</summary><p>Large Language Models (LLMs) have shown significant limitations in understanding creative content, as demonstrated by Hessel et al. (2023)'s influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study exposed a substantial gap between LLMs and humans in humor comprehension, establishing that understanding and evaluating creative content is key challenge in AI development. We revisit this challenge by decomposing humor understanding into three components and systematically improve each: enhancing visual understanding through improved annotation, utilizing LLM-generated humor reasoning and explanations, and implementing targeted alignment with human preference data. Our refined approach achieves 82.4% accuracy in caption ranking, singificantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts in this domain. Notably, while attempts to mimic subgroup preferences through various persona prompts showed minimal impact, model finetuning with crowd preferences proved remarkably effective. These findings reveal that LLM limitations in creative judgment can be effectively addressed through focused alignment to specific subgroups and individuals. Lastly, we propose the position that achieving artificial general intelligence necessitates systematic collection of human preference data across creative domains. We advocate that just as human creativity is deeply influenced by individual and cultural preferences, training LLMs with diverse human preference data may be essential for developing true creative understanding.</p></details> |  | <details><summary>Show</summary><p>该论文重新审视大语言模型理解创意内容的挑战，将幽默理解分解为三部分并逐一改进。通过改进标注等提升视觉理解，利用模型生成的推理和解释，用人群偏好微调模型。新方法在字幕排名中准确率达82.4%，超此前基准，还发现针对特定人群微调有效。 </p></details> |
| **[Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](http://arxiv.org/abs/2502.20339v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.</p></details> |  | <details><summary>Show</summary><p>该论文通过从预训练Transformer中蒸馏纯和混合Mamba模型，探讨低复杂度模型能否在固定计算预算下超越同类Transformer。虽蒸馏有零样本性能损失，但模型在数学推理数据集上表现强且推理快，为扩展推理计算开辟新方向。 </p></details> |
| **[KeBaB: $k$-mer based breaking for finding super-maximal exact matches](http://arxiv.org/abs/2502.20338v1)** | 2025-02-27 | <details><summary>Show</summary><p>Suppose we have a tool for finding super-maximal exact matches (SMEMs) and we want to use it to find all the long SMEMs between a noisy long read $P$ and a highly repetitive pangenomic reference $T$. Notice that if $L \geq k$ and the $k$-mer $P [i..i + k - 1]$ does not occur in $T$ then no SMEM of length at least $L$ contains $P [i..i + k - 1]$. Therefore, if we have a Bloom filter for the distinct $k$-mers in $T$ and we want to find only SMEMs of length $L \geq k$, then when given $P$ we can break it into maximal substrings consisting only of $k$-mers the filter says occur in $T$ -- which we call pseudo-SMEMs -- and search only the ones of length at least $L$. If $L$ is reasonably large and we can choose $k$ well then the Bloom filter should be small (because $T$ is highly repetitive) but the total length of the pseudo-SMEMs we search should also be small (because $P$ is noisy). Now suppose we are interested only in the longest $t$ SMEMs of length at least $L$ between $P$ and $T$. Notice that once we have found $t$ SMEMs of length at least $\ell$ then we need only search for SMEMs of length greater than $\ell$. Therefore, if we sort the pseudo-SMEMs into non-increasing order by length, then we can stop searching once we have found $t$ SMEMs at least as long as the next pseudo-SMEM we would search. Our preliminary experiments indicate that these two admissible heuristics may significantly speed up SMEM-finding in practice.</p></details> |  | <details><summary>Show</summary><p>该论文提出KeBaB方法，利用Bloom filter将长读P拆分为仅由在参考T中出现的k-mer组成的伪SMEMs，按长度降序排序，找到t个至少为L长度的SMEMs后停止搜索，可显著加速SMEM查找。 </p></details> |
| **[Expertise Is What We Want](http://arxiv.org/abs/2502.20335v1)** | 2025-02-27 | <details><summary>Show</summary><p>Clinical decision-making depends on expert reasoning, which is guided by standardized, evidence-based guidelines. However, translating these guidelines into automated clinical decision support systems risks inaccuracy and importantly, loss of nuance. We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of Large Language Models (LLMs) with the interpretability, explainability, and reliability of Expert Systems. LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability. To highlight the power of the Large Language Expert (LLE) system, we built an LLE to assist with the workup of patients newly diagnosed with cancer. Timely initiation of cancer treatment is critical for optimal patient outcomes. However, increasing complexity in diagnostic recommendations has made it difficult for primary care physicians to ensure their patients have completed the necessary workup before their first visit with an oncologist. As with many real-world clinical tasks, these workups require the analysis of unstructured health records and the application of nuanced clinical decision logic. In this study, we describe the design & evaluation of an LLE system built to rapidly identify and suggest the correct diagnostic workup. The system demonstrated a high degree of clinical-level accuracy (>95%) and effectively addressed gaps identified in real-world data from breast and colon cancer patients at a large academic center.</p></details> | <details><summary>18 pa...</summary><p>18 pages, 7 figures, 5 tables</p></details> | <details><summary>Show</summary><p>论文提出Large Language Expert（LLE）应用架构，结合大语言模型与专家系统优势。以辅助癌症初诊患者检查为例，展示其设计与评估，该系统临床准确率超95%，有效解决实际数据问题。 </p></details> |
| **[Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models](http://arxiv.org/abs/2502.20332v1)** | 2025-02-27 | <details><summary>Show</summary><p>Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.</p></details> |  | <details><summary>Show</summary><p>该论文对开源语言模型Llama3-70B中支持抽象规则归纳的内部机制进行全面研究，发现一种通过三步计算实现抽象推理的新兴符号架构，为符号与神经网络方法之争提供新解，表明神经网络中的新兴推理依赖符号机制的出现。 </p></details> |
| **[Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference](http://arxiv.org/abs/2407.00075v4)** | 2025-02-27 | <details><summary>Show</summary><p>We study how to subvert large language models (LLMs) from following prompt-specified rules. We first formalize rule-following as inference in propositional Horn logic, a mathematical system in which rules have the form "if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. Next, we prove that although small transformers can faithfully follow such rules, maliciously crafted prompts can still mislead both theoretical constructions and models learned from data. Furthermore, we demonstrate that popular attack algorithms on LLMs find adversarial prompts and induce attention patterns that align with our theory. Our novel logic-based framework provides a foundation for studying LLMs in rule-based settings, enabling a formal analysis of tasks like logical reasoning and jailbreak attacks.</p></details> |  | <details><summary>Show</summary><p>该论文研究如何使大语言模型违背提示规则。将规则遵循形式化为命题霍恩逻辑推理，证明恶意提示会误导模型，展示攻击算法能找到对抗性提示，其基于逻辑的新框架为研究基于规则的大语言模型提供基础。 </p></details> |
| **[Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases](http://arxiv.org/abs/2502.20317v1)** | 2025-02-27 | <details><summary>Show</summary><p>Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.</p></details> |  | <details><summary>Show</summary><p>该论文针对文本丰富图知识库，提出混合结构与文本检索（MoR）方法。通过规划 - 推理 - 组织框架，结合文本规划图、结构遍历与文本匹配及结构轨迹重排候选。实验证明其在协调两类检索上的优势，代码可获取。 </p></details> |
| **[EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants](http://arxiv.org/abs/2502.20309v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.</p></details> | 33 pages, 18 figures | <details><summary>Show</summary><p>论文介绍了阿贡国家实验室开发的EAIRA方法，用于评估AI模型作为科研助手。该方法包含四类评估：选择题测事实记忆、开放题评高级推理等能力、实验室实验及实地实验，能全面分析模型优缺点，且可不断演进，适用于多科研领域。 </p></details> |
| **[Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization](http://arxiv.org/abs/2502.20382v1)** | 2025-02-27 | <details><summary>Show</summary><p>We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline's effectiveness by training diffusion policies from the generated datasets for challenging contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bimanual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input. Project website: https://lujieyang.github.io/physicsgen/.</p></details> |  | <details><summary>Show</summary><p>该论文提出低成本数据生成管道，融合物理模拟、人类示范和基于模型的规划，从少量虚拟现实环境中的人类示范出发，经优化生成跨机器人实例和物理参数的高质量数据集，验证其有效性，实现跨实例数据转移及零样本硬件部署。 </p></details> |
| **[Tight Inversion: Image-Conditioned Inversion for Real Image Editing](http://arxiv.org/abs/2502.20376v1)** | 2025-02-27 | <details><summary>Show</summary><p>Text-to-image diffusion models offer powerful image editing capabilities. To edit real images, many methods rely on the inversion of the image into Gaussian noise. A common approach to invert an image is to gradually add noise to the image, where the noise is determined by reversing the sampling equation. This process has an inherent tradeoff between reconstruction and editability, limiting the editing of challenging images such as highly-detailed ones. Recognizing the reliance of text-to-image models inversion on a text condition, this work explores the importance of the condition choice. We show that a condition that precisely aligns with the input image significantly improves the inversion quality. Based on our findings, we introduce Tight Inversion, an inversion method that utilizes the most possible precise condition -- the input image itself. This tight condition narrows the distribution of the model's output and enhances both reconstruction and editability. We demonstrate the effectiveness of our approach when combined with existing inversion methods through extensive experiments, evaluating the reconstruction accuracy as well as the integration with various editing methods.</p></details> | <details><summary>Proje...</summary><p>Project page at: https://tight-inversion.github.io</p></details> | <details><summary>Show</summary><p>该论文针对文本到图像扩散模型编辑真实图像时依赖图像转高斯噪声的问题，探索条件选择重要性，引入Tight Inversion方法，用输入图像自身作条件，缩小模型输出分布，提升重建与可编辑性，实验验证其有效性。 </p></details> |
| **[Constrained Generative Modeling with Manually Bridged Diffusion Models](http://arxiv.org/abs/2502.20371v1)** | 2025-02-27 | <details><summary>Show</summary><p>In this paper we describe a novel framework for diffusion-based generative modeling on constrained spaces. In particular, we introduce manual bridges, a framework that expands the kinds of constraints that can be practically used to form so-called diffusion bridges. We develop a mechanism for combining multiple such constraints so that the resulting multiply-constrained model remains a manual bridge that respects all constraints. We also develop a mechanism for training a diffusion model that respects such multiple constraints while also adapting it to match a data distribution. We develop and extend theory demonstrating the mathematical validity of our mechanisms. Additionally, we demonstrate our mechanism in constrained generative modeling tasks, highlighting a particular high-value application in modeling trajectory initializations for path planning and control in autonomous vehicles.</p></details> | AAAI 2025 | <details><summary>Show</summary><p>该论文提出基于扩散模型的受限空间生成建模新框架，引入手动桥扩展可用约束类型，开发组合多约束及训练尊重多约束扩散模型的机制，论证其数学有效性，并在自动驾驶路径规划等任务中展示。 </p></details> |
| **[Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation](http://arxiv.org/abs/2502.20370v1)** | 2025-02-27 | <details><summary>Show</summary><p>This paper addresses the task of generating two-character online interactions. Previously, two main settings existed for two-character interaction generation: (1) generating one's motions based on the counterpart's complete motion sequence, and (2) jointly generating two-character motions based on specific conditions. We argue that these settings fail to model the process of real-life two-character interactions, where humans will react to their counterparts in real time and act as independent individuals. In contrast, we propose an online reaction policy, called Ready-to-React, to generate the next character pose based on past observed motions. Each character has its own reaction policy as its "brain", enabling them to interact like real humans in a streaming manner. Our policy is implemented by incorporating a diffusion head into an auto-regressive model, which can dynamically respond to the counterpart's motions while effectively mitigating the error accumulation throughout the generation process. We conduct comprehensive experiments using the challenging boxing task. Experimental results demonstrate that our method outperforms existing baselines and can generate extended motion sequences. Additionally, we show that our approach can be controlled by sparse signals, making it well-suited for VR and other online interactive environments.</p></details> | <details><summary>Accep...</summary><p>Accepted as ICLR 2025 conference paper</p></details> | <details><summary>Show</summary><p>该论文针对两角色在线交互生成任务，指出以往设置无法模拟真实交互过程。提出“Ready-to-React”在线反应策略，通过将扩散头融入自回归模型实现，能实时响应并优于基线，可生成扩展动作序列，适用于VR等在线交互环境。 </p></details> |
| **[G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation](http://arxiv.org/abs/2411.18369v2)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advances in imitation learning for 3D robotic manipulation have shown promising results with diffusion-based policies. However, achieving human-level dexterity requires seamless integration of geometric precision and semantic understanding. We present G3Flow, a novel framework that constructs real-time semantic flow, a dynamic, object-centric 3D semantic representation by leveraging foundation models. Our approach uniquely combines 3D generative models for digital twin creation, vision foundation models for semantic feature extraction, and robust pose tracking for continuous semantic flow updates. This integration enables complete semantic understanding even under occlusions while eliminating manual annotation requirements. By incorporating semantic flow into diffusion policies, we demonstrate significant improvements in both terminal-constrained manipulation and cross-object generalization. Extensive experiments across five simulation tasks show that G3Flow consistently outperforms existing approaches, achieving up to 68.3% and 50.1% average success rates on terminal-constrained manipulation and cross-object generalization tasks respectively. Our results demonstrate the effectiveness of G3Flow in enhancing real-time dynamic semantic feature understanding for robotic manipulation policies.</p></details> | <details><summary>Webpa...</summary><p>Webpage: https://tianxingchen.github.io/G3Flow/, accepted to CVPR 2025</p></details> | <details><summary>Show</summary><p>该论文提出G3Flow框架，利用基础模型构建实时语义流，结合3D生成模型、视觉基础模型和姿态跟踪实现语义理解。无需手动标注，融入扩散策略提升操作性能，实验表明其在多任务中优于现有方法，增强了机器人操作的语义特征理解。 </p></details> |
| **[ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model](http://arxiv.org/abs/2502.20323v1)** | 2025-02-27 | <details><summary>Show</summary><p>Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips. Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential. In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook. Furthermore, our model can adapt to unseen speaking styles using sample motion sequences, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality.</p></details> | <details><summary>More ...</summary><p>More video demonstrations, code, models and data can be found on our project website: http://xg-chu.site/project_artalk/</p></details> | <details><summary>Show</summary><p>该论文提出一种基于自回归模型的ARTalk，用于语音驱动3D头部动画。能实时生成高度同步的嘴唇动作、逼真头部姿势和眨眼，可适应未见说话风格，在唇同步精度和感知质量上优于现有方法。 </p></details> |
| **[FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction](http://arxiv.org/abs/2502.20313v1)** | 2025-02-27 | <details><summary>Show</summary><p>This work challenges the residual prediction paradigm in visual autoregressive modeling and presents FlexVAR, a new Flexible Visual AutoRegressive image generation paradigm. FlexVAR facilitates autoregressive learning with ground-truth prediction, enabling each step to independently produce plausible images. This simple, intuitive approach swiftly learns visual distributions and makes the generation process more flexible and adaptable. Trained solely on low-resolution images ($\leq$ 256px), FlexVAR can: (1) Generate images of various resolutions and aspect ratios, even exceeding the resolution of the training images. (2) Support various image-to-image tasks, including image refinement, in/out-painting, and image expansion. (3) Adapt to various autoregressive steps, allowing for faster inference with fewer steps or enhancing image quality with more steps. Our 1.0B model outperforms its VAR counterpart on the ImageNet 256$\times$256 benchmark. Moreover, when zero-shot transfer the image generation process with 13 steps, the performance further improves to 2.08 FID, outperforming state-of-the-art autoregressive models AiM/VAR by 0.25/0.28 FID and popular diffusion models LDM/DiT by 1.52/0.19 FID, respectively. When transferring our 1.0B model to the ImageNet 512$\times$512 benchmark in a zero-shot manner, FlexVAR achieves competitive results compared to the VAR 2.3B model, which is a fully supervised model trained at 512$\times$512 resolution.</p></details> |  | <details><summary>Show</summary><p>论文提出FlexVAR这一灵活的视觉自回归图像生成范式，挑战残差预测模式，以真实预测促进自回归学习。仅在低分辨率图像上训练，就能生成多样分辨率等，支持多种任务，性能超越同类及扩散模型。 </p></details> |
| **[Mobius: Text to Seamless Looping Video Generation via Latent Shift](http://arxiv.org/abs/2502.20307v1)** | 2025-02-27 | <details><summary>Show</summary><p>We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training. During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process. Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model's context. Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality. We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios. All the code will be made available.</p></details> | <details><summary>Proje...</summary><p>Project page: https://mobius-diffusion.github.io/ ; GitHub repository: https://github.com/YisuiTT/Mobius</p></details> | <details><summary>Show</summary><p>论文提出Mobius方法，无需用户标注，直接根据文本描述生成无缝循环视频。利用预训练视频潜扩散模型，通过构建潜循环、逐帧潜去噪实现，能生成任意长度循环视频，效果优于以往方法，实验验证了其有效性。 </p></details> |
| **[R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts](http://arxiv.org/abs/2502.20395v1)** | 2025-02-27 | <details><summary>Show</summary><p>In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method "Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.</p></details> |  | <details><summary>Show</summary><p>论文指出大型多模态模型中视觉等非语言模态感知能力弱，影响下游任务表现。提出“R2-T2”方法，在测试时局部优化路由权重向量，通过三种策略提升模型性能，且无需训练基础模型参数。 </p></details> |
| **[Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](http://arxiv.org/abs/2502.20383v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.</p></details> | <details><summary>Proje...</summary><p>Project website: http://vulnerable-ai-agents.github.io</p></details> | <details><summary>Show</summary><p>该论文研究为何网络人工智能代理比独立语言模型更易受攻击。通过调查发现，其脆弱性源于与独立模型的多方面差异及复杂信号。提出组件级分析和系统评估框架，找出三个关键因素，强调提升人工智能代理设计安全性的迫切性。 </p></details> |
| **[Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers](http://arxiv.org/abs/2502.20379v1)** | 2025-02-27 | <details><summary>Show</summary><p>By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.</p></details> |  | <details><summary>Show</summary><p>该论文提出在测试时通过增加验证器数量来扩展计算，引入多智能体验证（MAV）范式，使用现成的方面验证器（AVs），还介绍了BoN-MAV算法，证明其在提升语言模型性能上比其他方法更具优势，是测试时计算的新有效维度。 </p></details> |
| **[PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation](http://arxiv.org/abs/2502.20377v1)** | 2025-02-27 | <details><summary>Show</summary><p>High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at https://github.com/kilian-group/phantom-wiki.</p></details> |  | <details><summary>Show</summary><p>该论文提出PhantomWiki，是用于推理和检索评估的按需生成数据集的管道。它能生成独特、事实一致的语料库及问答对，每次评估按需生成新实例，可分别评估推理和检索能力，贡献了抗数据泄露的可扩展评估框架。 </p></details> |
| **[Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](http://arxiv.org/abs/2502.20364v1)** | 2025-02-27 | <details><summary>Show</summary><p>Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.</p></details> | <details><summary>10 pa...</summary><p>10 pages, 6 figures, 5 tables</p></details> | <details><summary>Show</summary><p>该论文介绍了一种将检索增强生成（RAG）、向量存储（VS）和知识图谱（KG）集成的生成式人工智能系统，通过非负矩阵分解构建，用于法律信息检索与AI推理，减少幻觉，提升法律研究效率，推动计算法学和AI发展。 </p></details> |
| **[Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs](http://arxiv.org/abs/2502.20356v1)** | 2025-02-27 | <details><summary>Show</summary><p>Large Language Models (LLMs) have shown significant limitations in understanding creative content, as demonstrated by Hessel et al. (2023)'s influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study exposed a substantial gap between LLMs and humans in humor comprehension, establishing that understanding and evaluating creative content is key challenge in AI development. We revisit this challenge by decomposing humor understanding into three components and systematically improve each: enhancing visual understanding through improved annotation, utilizing LLM-generated humor reasoning and explanations, and implementing targeted alignment with human preference data. Our refined approach achieves 82.4% accuracy in caption ranking, singificantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts in this domain. Notably, while attempts to mimic subgroup preferences through various persona prompts showed minimal impact, model finetuning with crowd preferences proved remarkably effective. These findings reveal that LLM limitations in creative judgment can be effectively addressed through focused alignment to specific subgroups and individuals. Lastly, we propose the position that achieving artificial general intelligence necessitates systematic collection of human preference data across creative domains. We advocate that just as human creativity is deeply influenced by individual and cultural preferences, training LLMs with diverse human preference data may be essential for developing true creative understanding.</p></details> |  | <details><summary>Show</summary><p>该论文针对大语言模型在理解创意内容上的局限，将幽默理解分解为三部分并改进：通过改进标注等提升视觉理解等。经优化方法在标题排名准确率达82.4%，超此前基准，发现针对特定人群微调有效，还提出实现通用人工智能需系统收集人类偏好数据。 </p></details> |
| **[KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model](http://arxiv.org/abs/2502.20350v1)** | 2025-02-27 | <details><summary>Show</summary><p>Drug discovery is a critical task in biomedical natural language processing (NLP), yet explainable drug discovery remains underexplored. Meanwhile, large language models (LLMs) have shown remarkable abilities in natural language understanding and generation. Leveraging LLMs for explainable drug discovery has the potential to improve downstream tasks and real-world applications. In this study, we utilize open-source drug knowledge graphs, clinical trial data, and PubMed publications to construct a comprehensive dataset for the explainable drug discovery task, named \textbf{expRxRec}. Furthermore, we introduce \textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge from rich medical knowledge corpus for drug recommendation and rationale generation. To encourage further research in this area, we will publicly release\footnote{A copy is attached with this submission} both the dataset and KEDRec-LM.</p></details> |  | <details><summary>Show</summary><p>该论文聚焦生物医学NLP中药物发现任务，利用多源数据构建数据集expRxRec。创新地提出KEDRec-LM，这是经指令微调、从医学知识语料库提取知识用于药物推荐及理由生成的语言模型，还将公开数据集和模型以促研究。 </p></details> |
| **[Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models](http://arxiv.org/abs/2502.20344v1)** | 2025-02-27 | <details><summary>Show</summary><p>Large language models (LLMs) excel in tasks that require complex linguistic abilities, such as reference disambiguation and metaphor recognition/generation. Although LLMs possess impressive capabilities, their internal mechanisms for processing and representing linguistic knowledge remain largely opaque. Previous work on linguistic mechanisms has been limited by coarse granularity, insufficient causal analysis, and a narrow focus. In this study, we present a systematic and comprehensive causal investigation using sparse auto-encoders (SAEs). We extract a wide range of linguistic features from six dimensions: phonetics, phonology, morphology, syntax, semantics, and pragmatics. We extract, evaluate, and intervene on these features by constructing minimal contrast datasets and counterfactual sentence datasets. We introduce two indices-Feature Representation Confidence (FRC) and Feature Intervention Confidence (FIC)-to measure the ability of linguistic features to capture and control linguistic phenomena. Our results reveal inherent representations of linguistic knowledge in LLMs and demonstrate the potential for controlling model outputs. This work provides strong evidence that LLMs possess genuine linguistic knowledge and lays the foundation for more interpretable and controllable language modeling in future research.</p></details> |  | <details><summary>Show</summary><p>该论文用稀疏自编码器对大语言模型的语言机制进行系统全面因果研究。从多维度提取语言特征，构建数据集并引入指标衡量。揭示模型语言知识内在表征，为未来更具可解释性和可控性的语言建模奠定基础。 </p></details> |
| **[Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](http://arxiv.org/abs/2502.20339v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.</p></details> |  | <details><summary>Show</summary><p>该论文探讨能否用低复杂度模型在固定计算预算下超越同等规模Transformer。通过从预训练Transformer中蒸馏出纯和混合Mamba模型，虽蒸馏使零样本性能有损失，但新模型在数学推理数据集上表现强且推理快，为扩展推理计算开辟新方向。 </p></details> |
| **[Expertise Is What We Want](http://arxiv.org/abs/2502.20335v1)** | 2025-02-27 | <details><summary>Show</summary><p>Clinical decision-making depends on expert reasoning, which is guided by standardized, evidence-based guidelines. However, translating these guidelines into automated clinical decision support systems risks inaccuracy and importantly, loss of nuance. We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of Large Language Models (LLMs) with the interpretability, explainability, and reliability of Expert Systems. LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability. To highlight the power of the Large Language Expert (LLE) system, we built an LLE to assist with the workup of patients newly diagnosed with cancer. Timely initiation of cancer treatment is critical for optimal patient outcomes. However, increasing complexity in diagnostic recommendations has made it difficult for primary care physicians to ensure their patients have completed the necessary workup before their first visit with an oncologist. As with many real-world clinical tasks, these workups require the analysis of unstructured health records and the application of nuanced clinical decision logic. In this study, we describe the design & evaluation of an LLE system built to rapidly identify and suggest the correct diagnostic workup. The system demonstrated a high degree of clinical-level accuracy (>95%) and effectively addressed gaps identified in real-world data from breast and colon cancer patients at a large academic center.</p></details> | <details><summary>18 pa...</summary><p>18 pages, 7 figures, 5 tables</p></details> | <details><summary>Show</summary><p>论文提出一种结合大语言模型（LLMs）与专家系统优势的大型语言专家（LLE）应用架构，以解决临床决策支持系统的问题。构建LLE辅助癌症初诊患者检查，设计并评估该系统，其临床准确率超95%，有效填补实际数据差距。 </p></details> |
| **[Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models](http://arxiv.org/abs/2502.20332v1)** | 2025-02-27 | <details><summary>Show</summary><p>Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.</p></details> |  | <details><summary>Show</summary><p>该论文对开源语言模型Llama3-70B中支持抽象规则归纳的内部机制进行全面研究，发现一种通过三步计算实现抽象推理的新兴符号架构，为符号与神经网络方法之争提供新视角，表明神经网络中的新兴推理依赖符号机制。 </p></details> |
| **[Long-Context Inference with Retrieval-Augmented Speculative Decoding](http://arxiv.org/abs/2502.20330v1)** | 2025-02-27 | <details><summary>Show</summary><p>The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference, particularly in managing key-value (KV) caches, presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer dynamic that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both approaches, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups. Our analyses reveal that RAPID achieves robust acceleration beyond 32K context length and demonstrates superior generation quality in real-world applications.</p></details> |  | <details><summary>Show</summary><p>该论文提出检索增强推测解码（RAPID），利用RAG加速长上下文推理并提升生成质量。引入RAG草稿模型，开发推理时知识转移动态。实验表明RAPID有效整合优势，大幅提升性能与速度，超32K上下文长度也能加速且质量优。 </p></details> |
| **[Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference](http://arxiv.org/abs/2407.00075v4)** | 2025-02-27 | <details><summary>Show</summary><p>We study how to subvert large language models (LLMs) from following prompt-specified rules. We first formalize rule-following as inference in propositional Horn logic, a mathematical system in which rules have the form "if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. Next, we prove that although small transformers can faithfully follow such rules, maliciously crafted prompts can still mislead both theoretical constructions and models learned from data. Furthermore, we demonstrate that popular attack algorithms on LLMs find adversarial prompts and induce attention patterns that align with our theory. Our novel logic-based framework provides a foundation for studying LLMs in rule-based settings, enabling a formal analysis of tasks like logical reasoning and jailbreak attacks.</p></details> |  | <details><summary>Show</summary><p>该论文研究如何颠覆大语言模型遵循提示规则的情况。将规则遵循形式化为命题霍恩逻辑推理，证明恶意提示会误导模型，展示攻击算法能找到对抗提示，其基于逻辑的新框架为研究规则设定下的大语言模型提供基础。 </p></details> |
| **[LangProBe: a Language Programs Benchmark](http://arxiv.org/abs/2502.20315v1)** | 2025-02-27 | <details><summary>Show</summary><p>Composing language models (LMs) into multi-step language programs and automatically optimizing their modular prompts is now a mainstream paradigm for building AI systems, but the tradeoffs in this space have only scarcely been studied before. We introduce LangProBe, the first large-scale benchmark for evaluating the architectures and optimization strategies for language programs, with over 2000 combinations of tasks, architectures, optimizers, and choices of LMs. Using LangProBe, we are the first to study the impact of program architectures and optimizers (and their compositions together and with different models) on tradeoffs of quality and cost. We find that optimized language programs offer strong cost--quality Pareto improvement over raw calls to models, but simultaneously demonstrate that human judgment (or empirical decisions) about which compositions to pursue is still necessary for best performance. We will open source the code and evaluation data for LangProBe.</p></details> |  | <details><summary>Show</summary><p>论文介绍了首个用于评估语言程序架构和优化策略的大规模基准LangProBe，有超2000种任务等组合。首次研究程序架构等对质量与成本权衡的影响，发现优化程序有帕累托改进，同时表明人工判断仍必要，还将开源代码和数据。 </p></details> |
| **[EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants](http://arxiv.org/abs/2502.20309v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.</p></details> | 33 pages, 18 figures | <details><summary>Show</summary><p>该论文介绍了阿贡国家实验室开发的EAIRA方法，用于评估AI模型作为科研助手。它包含四类评估：选择题测事实记忆、开放回答评估高级推理等，还有实验室及实地实验。能全面分析模型优缺点，且可随LLM发展而演变，具广泛适用性。 </p></details> |
| **[Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](http://arxiv.org/abs/2502.20339v1)** | 2025-02-27 | <details><summary>Show</summary><p>Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.</p></details> |  | <details><summary>Show</summary><p>该论文探讨能否用低复杂度模型在固定计算预算下超越同等规模Transformer。为此从预训练Transformer中蒸馏出纯和混合Mamba模型，虽蒸馏致零样本性能下降，但在数学推理数据集上表现佳且推理快，为扩展推理计算开辟新方向。 </p></details> |

